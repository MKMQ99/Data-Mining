<center><h1>文档</h1></center>

[toc]



### 1. 数据集概况

​		本小组采用的数据集采集于 `TripAdvisor.com` ，数据为旅行者对东亚地区10个类别的目的地进行的评估。利用这些数据，可以对用户进行聚类，从而得知哪些用户具有共同的旅游爱好。



### 2. 数据预处理

#### 2.1 了解数据的概况

​		使用 `pandas.read_csv()` 函数导入 `csv` 文件并使用 `info()` 函数查看数据的概况，发现共有11个属性，980条数据。其中第一个属性为 `user id ` ，对数据分析并无用处，所以之后的分析将不对这一列进行处理。

<img src=".\图片\数据概况.png" style="zoom: 80%;" />

<center>图1：数据集概况</center>

可以看到并没有数据丢失，再来看看每个数据的值大概是多少，有没有离群点。先画个散点图粗略地看一看。我们使用了`plt.scatter()` 画图，具体代码和图片如下：

```python
#画出10个属性索引与数值的散点图，观察数据大致的分布
plt.rcParams['figure.figsize'] = (20, 8) #设置画布大小为20*8
fig, ax = plt.subplots(2,5) #设置子图，为2*5方哥摆布
plt.subplots_adjust(wspace=0.2, hspace=0.3)
#画出每个属性的散点图
for i in range(10):
    num = i+1
    plt.subplot(2,5,num)
    plt.scatter(df.index, df[df.columns[i+1]], c="b", label=str(df.columns[i+1]), s=3)
    plt.ylabel("数值")
    plt.xlabel("索引")
    plt.title(df.columns[i+1])
plt.show()
```

![](.\图片\散点图.png)

<center>图2：数据值查看</center>

发现确实存在离群点，接下来就对离群点进行处理。



#### 2.2 离群点的处理

​		我们先仔细观察离群点，使用箱体图具体观察。选用箱体图，是因为箱体图可以反映原始数据分布的特征，并方便的确定离群点。具体代码和图片如下：

```python
#由散点图发现存在离群点，用箱体图能更加细致地查看
#第一个属性"user id"在数据分析中并没有什么作用，所以使用去掉"user id"属性的data进行下一步分析
labels = ["art galleries", "dance clubs", "juice bars", "restaurants", "museums", "resorts", "parks/picnic spots", "beaches", "theaters", "religious institutions"]
data = df[labels]
box = []
#绘制箱体图
for i in range(10):
    box.append(data[data.columns[i]])
plt.title('各属性箱体图',fontsize=20) #标题，并设定字号大小
p = plt.boxplot(box, labels = labels, patch_artist = True, boxprops = {'color':'orangered','facecolor':'pink'})
```

![](.\图片\原数据箱体图.png)

<center>图3：各属性箱体图</center>

​		在箱体图中，具体可以看到第1个属性到第9个属性，或多或少都具有离群点，只有最后一个属性不存在离群点。但是第7个属性数据的分布由图中查看显然非常的集中，统计的离群点应该是有点问题的。浏览了具体的数据后发现是因为数值之间差值很小，又因为取值也就几个，比较集中，所以这个属性的数值没必要做修改。
​		接下来我们对离群点做处理，我们的想法是设置大于上限的值为最大值，小于下限的值为最小值，从而修改离群点为正常点。这里我们设计了一个函数 `BoxFeature()`：用于获得箱体图的具体信息：

```python
#BoxFeature函数用于获取箱体图的具体信息
def BoxFeature(input_list):
    # 获取箱体图特征
    percentile = np.percentile(input_list, (25, 50, 75), interpolation='linear')
    #以下为箱线图的五个特征值
    Q1 = percentile[0] #上四分位数
    Q2= percentile[1]
    Q3 = percentile[2] #下四分位数
    IQR = Q3 - Q1 #四分位距
    ulim = Q3 + 1.5 * IQR #上限 非异常范围内的最大值
    llim = Q1 - 1.5 * IQR #下限 非异常范围内的最小值
    right_list = [] #正常数据列表
    Error_Point_num = 0 #异常点个数
    value_total = 0 
    average_num = 0 #平均值
    for item in input_list:
        if item < llim or item > ulim:
            Error_Point_num += 1
        else:
            right_list.append(item)
        value_total += item
        average_num += 1
    average_value = value_total/average_num
    # 特征值保留一位小数
    out_list = [average_value,min(right_list), Q1, Q2, Q3, max(right_list)]
    # print(out_list)
    return out_list,Error_Point_num
```

​		利用这个函数，我们获得箱体图正常的最大最小值，从而对离群点进行修改，具体代码和修改后的箱体图如下：

```python
#设置大于上限的值为最大值，小于下限的值为最小值，从而修改离群点为正常点
for i in range(10):
    if i == 6: #第7个属性因为数据相当集中，差值较小，不必更改
        continue
    name = data.columns[i]
    #调用BoxFeature函数获得箱体图信息
    out_list,Error_Point_num = BoxFeature(data[name])
    maxnum = out_list[5] 
    minnum = out_list[1]
    for j in range(980):
        if data.iat[j,i]> maxnum :
            data.iat[j,i] = maxnum
        elif data.iat[j,i] < minnum:
            data.iat[j,i] = minnum   
labels = ["art galleries", "dance clubs", "juice bars", "restaurants", "museums", "resorts", "parks/picnic spots", "beaches", "theaters", "religious institutions"]
box = []
for i in range(10):
    box.append(data[data.columns[i]])
plt.title('Examples of boxplot',fontsize=20)#标题，并设定字号大小
p = plt.boxplot(box, labels = labels, patch_artist = True, boxprops = {'color':'orangered','facecolor':'pink'})
```

![](.\图片\处理后箱体图.png)

<center>图4：去除离群点后箱体图</center>

可以发现，除了属性7，其余属性中，已经不存在离群点，效果很好。



#### 2.3 数据规范化

​		接下来，我们对数据进行了归一化，方便后续处理，以获得更好的聚类效果。这里我们采用的是 `Z-score` 规范化。因为我们在图1中其实可以看出来，各属性数值的分布基本是中间集中，往两边逐渐减少，类似正态分布。从图4中也可以看出这样的趋势。不过，从前期可以看出，这些属性里，第三个属性 `juice bars` 的大量数据集中在较小值，可能不符合这个规律。
​		我们对数据使用 `Z-score` 规范化后，再绘制直方图可观察数据分布，具体代码和图像如下：

```python
#规范化数据，方便之后的处理
fig, ax = plt.subplots(2,5)
plt.subplots_adjust(wspace=0.2, hspace=0.3)
values = data.astype(float) #dataframe转换为array
g = preprocessing.scale(values, axis=0, with_mean=True, with_std=True, copy=True)
data = pd.DataFrame(g)
data.columns = ["art galleries", "dance clubs", "juice bars", "restaurants", "museums", "resorts", "parks/picnic spots", "beaches", "theaters", "religious institutions"]
for i in range(10):
    plt.subplot(2,5,i+1)
    plt.hist(data[data.columns[i]], bins=100)
    plt.ylabel("数目")
    plt.xlabel("数值")
    plt.title(data.columns[i])
plt.show()
```

![](.\图片\直方图.png)

<center>图5：各属性分布直方图</center>

从图片中可以观察到，确实除了第3个属性 `juice bars` 不太符合之外，其余属性规范化的效果都很好。



#### 2.4 属性相关性判断

​		为了防止属性冗余，我们需要对各个属性间的相关性进行判断。这里我们采用散点图矩阵方便的判断。具体代码和图片如下：

```python
#查看每个属性之间是否存在关系，由相关性散点图可以看出来，10个属性之间的关系很小
pd.plotting.scatter_matrix(data, figsize=(20,20), c = 'k', marker = '0', s = 3, diagonal='hist', alpha = 0.8, range_padding=0.2)
plt.show()
```

![](.\图片\散点矩阵.png)

<center>图6：散点图矩阵</center>

从散点图矩阵中，我们可以发现，各个属性的数值之间并没有呈现出线性的关系，可以得出结论：属性间的相关性小，没有冗余的属性。



### 3.聚类处理

#### 3.1二维下的聚类处理

##### 3.1.1 降维处理

​		出于减少计算量，进而提高机器运作效率的目的，我们往往会对数据进行降维处理。所以尽管3维数据不是必须要降维，这里还是将数据降到2维，来实验降维的功能。

```python
#进行降维
pca = PCA(n_components=2)
newdata = pca.fit_transform(data)

x = []
y = []

for i in range(len(newdata)):
    x.append(newdata[i][0])
    y.append(newdata[i][1])

plt.scatter(x, y)
plt.show()
#降到2维的结果
```

​		可视化结果如下：

![](.\图片\2维处理后的散点图.png)

<center>图7：降维处理后的散点图</center>

##### 3.1.2 通过手肘法选择合适的k值

​		计算**SSE(误差平方和)**，以此选择出最理想的K值

```python
SSE = [] # 存放每次结果的误差平方和
for k in range(1,9): 
    estimator = KMeans(n_clusters=k, max_iter=100, init='k-means++')
    estimator.fit_predict(newdata_2) # 在多个K的取值下分别进行K-means处理
    SSE.append(estimator.inertia_)
X = range(1,9)
plt.xlabel('k')
plt.ylabel('SSE')
plt.plot(X,SSE,'o-')
plt.show()
```

![](.\图片\2维下SSE值曲线.png)

<center>图8：2维下的SEE值曲线</center>

​		从图中我们可以看到随着聚类数k的增大，样本划分会更加精细，每个簇的聚合程度会逐渐提高，那么误差平方和SSE自然会逐渐变小。并且，当k小于真实聚类数时，由于k的增大会大幅增加每个簇的聚合程度，故SSE的下降幅度会很大，而当k到达真实聚类数时，再增加k所得到的聚合程度回报会迅速变小，所以SSE的下降幅度会骤减，然后随着k值的继续增大而趋于平缓，也就是说图中斜率最大的点对应的k值就是数据的真实聚类数，图中这个点对应的K值为2。

##### 3.1.3 K=2的K-means聚类

​		使用K=2的sklearn K-means对数据进行聚类处理。

```python
km_cluster = KMeans(n_clusters=2, max_iter=100, init='k-means++')
#K=2，最多迭代100次，按k-means++选初始点
predictions = km_cluster.fit_predict(newdata)
print('result:', predictions)
print('clusters: first classes: %d, second classes: %d' %
      (np.sum(predictions == 0), np.sum(predictions == 1)))
#聚类结果，以及每个类包含的数据个数
```

```
result: 
[0 0 1 1 0 1 1 1 0 1 0 1 0 0 0 0 1 0 0 0 1 1 1 1 0 0 0 0 1 0 0 0 0 0 1 1 1
 1 0 1 1 1 1 1 1 1 1 1 1 0 1 1 0 1 1 1 0 1 0 0 0 1 1 1 1 1 1 1 1 1 1 0 1 1
 0 0 1 0 1 1 1 1 1 1 1 1 0 1 0 1 1 1 0 0 0 1 0 1 0 1 1 1 0 1 1 0 0 1 1 0 0
 1 0 0 0 0 0 1 1 1 0 0 0 0 0 1 1 0 1 1 1 0 0 0 1 0 0 0 1 1 1 1 1 1 1 1 0 0
 0 0 1 1 1 1 1 1 1 1 0 1 1 1 0 1 1 1 0 1 0 1 0 1 1 1 1 0 1 1 1 1 1 0 0 1 0
 1 1 1 0 1 0 0 1 1 1 1 1 0 1 0 0 1 0 1 1 1 1 1 1 1 1 0 1 1 1 0 1 1 0 0 0 1
 0 1 0 1 0 1 1 0 1 0 1 1 0 1 0 1 0 1 0 0 0 1 1 1 1 0 1 1 0 1 0 0 1 1 1 0 1
 0 0 0 1 0 1 0 0 0 1 0 1 1 0 0 0 1 0 0 1 1 1 1 0 0 1 1 0 1 1 1 0 1 1 1 1 0
 1 1 0 0 1 0 0 1 0 1 1 0 1 0 1 1 1 0 0 1 0 1 0 1 1 1 0 1 1 0 0 0 0 0 1 0 1
 0 1 0 0 1 0 1 0 0 1 0 1 1 0 1 0 0 1 1 0 1 1 1 0 1 0 1 1 0 0 1 0 0 0 1 1 0
 1 0 0 0 1 0 0 1 1 0 1 1 0 1 1 0 0 0 1 1 0 1 0 1 1 0 1 1 1 0 1 0 1 1 0 0 1
 0 0 0 1 0 0 0 0 0 1 1 1 0 0 1 0 1 1 0 0 1 1 1 0 0 0 0 0 1 1 0 1 1 1 1 1 1
 0 1 0 1 0 0 1 1 1 0 1 1 1 1 0 1 1 1 1 1 1 0 1 1 1 1 0 1 1 0 1 0 1 1 0 0 1
 0 1 0 0 1 1 1 1 0 1 1 1 0 1 1 1 0 0 1 0 1 1 1 1 1 1 0 0 1 0 1 1 0 1 0 1 0
 1 1 1 0 0 1 0 0 0 1 1 1 0 1 0 1 1 1 0 0 0 1 1 1 1 1 0 1 1 1 0 1 1 0 1 0 0
 0 1 1 1 1 0 0 0 1 0 1 0 0 0 1 0 1 0 1 0 0 1 1 1 0 1 0 0 1 1 1 1 1 0 1 1 1
 1 0 1 1 1 0 0 1 1 0 0 1 0 0 1 1 0 1 1 0 1 1 1 1 1 1 1 1 1 0 1 1 1 1 0 1 1
 1 1 1 1 0 1 0 0 0 1 1 0 0 0 1 0 1 1 1 1 0 1 1 1 1 0 0 1 0 1 1 0 1 0 1 1 1
 0 0 1 1 1 0 1 1 1 1 1 0 1 1 1 1 0 1 0 0 1 1 1 0 1 1 1 1 0 0 1 1 1 1 1 0 0
 1 1 1 1 0 1 1 0 0 1 1 1 1 1 0 0 0 0 1 0 1 1 0 0 0 0 0 1 1 1 0 1 1 0 0 0 1
 0 0 1 1 1 0 1 0 1 1 0 1 0 1 0 1 1 1 1 1 0 0 1 1 0 1 1 0 0 1 1 1 1 1 1 0 1
 0 1 1 0 0 1 1 1 0 0 1 1 0 1 0 0 1 1 0 1 1 0 0 1 0 0 1 0 0 1 0 0 0 0 0 0 1
 0 1 0 0 0 0 0 1 1 0 0 0 0 1 0 0 1 1 1 1 0 0 0 0 1 1 1 1 1 1 1 0 1 1 1 1 1
 0 0 0 0 1 0 0 1 0 0 1 0 1 1 1 0 0 1 1 0 1 1 1 1 0 0 1 1 0 1 1 0 0 0 0 1 1
 0 1 0 1 0 1 1 1 0 1 1 1 1 0 0 0 1 0 1 0 1 1 1 0 1 0 0 0 0 1 1 1 0 1 1 1 0
 0 0 1 0 1 1 0 1 1 1 1 1 0 0 0 0 1 0 1 1 1 1 1 0 0 1 0 0 0 0 0 0 1 1 1 0 1
 1 0 0 1 0 1 1 1 1 1 1 0 0 1 1 1 1 0]
clusters: first classes: 414, second classes: 566
```

​		可视化处理后的结果：

![](.\图片\2维数据K=2的K-means聚类.png)

<center>图9：K=2的K-means聚类</center>

##### 3.1.4 K=3的K-means聚类

​		使用K=3的sklearn K-means对数据进行聚类处理。

```python
km_cluster = KMeans(n_clusters=3, max_iter=300, init='k-means++')
#K=3，最多迭代300次，按k-means++选初始点
predictions = km_cluster.fit_predict(newdata)
print('result:', predictions)
print('clusters: first classes: %d, second classes: %d, thrid classes: %d' %
      (np.sum(predictions == 0), np.sum(predictions == 1), np.sum(predictions == 2)))
#聚类结果，以及每个类包含的数据个数
```

```
result: 
[2 2 0 1 2 1 1 1 2 1 0 1 2 2 2 2 1 2 0 2 1 1 1 1 0 2 2 2 1 2 2 2 2 0 0 0 1
 1 2 0 1 1 1 0 0 1 0 1 1 2 1 1 2 0 1 1 2 1 2 2 2 1 1 1 0 0 1 1 1 1 0 2 1 1
 2 2 1 2 1 1 0 0 1 0 0 1 0 1 2 1 0 1 2 2 2 0 2 0 0 0 0 1 2 1 1 2 0 1 1 2 0
 1 0 0 2 0 2 0 1 1 2 0 2 0 2 1 1 2 1 1 1 2 2 2 1 0 2 2 1 1 1 1 1 0 0 1 0 2
 0 2 0 1 1 1 1 1 0 1 2 1 1 1 0 1 1 1 2 0 2 0 0 0 1 1 0 2 0 0 0 0 0 2 0 1 2
 0 0 0 2 1 2 2 1 1 1 1 1 2 1 2 2 0 2 0 1 1 1 0 1 1 1 2 1 1 1 2 1 1 2 0 0 1
 2 1 2 1 2 1 1 2 0 2 1 1 0 1 0 0 0 1 2 2 0 1 1 1 0 2 1 1 2 1 2 2 1 0 1 2 0
 2 0 0 0 0 1 2 2 2 1 2 1 0 2 2 2 1 2 0 1 0 1 0 2 2 1 1 2 0 1 1 2 1 1 0 1 0
 1 1 0 2 1 2 2 1 0 1 0 2 0 2 1 1 1 2 2 0 2 1 2 1 0 0 2 1 0 2 2 2 2 0 0 2 0
 0 1 2 2 1 0 0 2 2 0 2 1 1 0 0 2 2 0 0 0 1 1 1 0 1 0 1 1 2 2 1 0 2 0 0 0 2
 1 2 0 2 0 2 2 1 0 2 1 1 0 1 1 2 2 0 1 1 2 1 2 1 0 2 1 0 1 0 0 2 0 1 2 2 1
 2 2 2 1 2 2 2 0 2 1 1 1 2 2 1 2 1 1 0 2 1 1 1 0 0 2 2 2 1 1 0 0 0 1 1 1 0
 2 1 2 1 2 0 1 1 0 2 0 1 0 1 2 1 0 1 0 0 0 2 1 0 1 1 2 1 1 0 1 0 1 1 2 2 0
 2 1 2 2 1 0 0 1 0 1 1 1 2 0 1 1 2 0 0 2 0 1 0 0 1 0 2 2 1 2 1 0 2 1 2 0 2
 1 0 1 2 2 1 2 2 2 0 1 0 0 0 2 1 1 1 2 2 2 0 1 1 0 1 2 1 1 0 2 1 0 2 0 0 2
 2 1 0 1 0 2 2 0 1 0 1 2 2 2 1 2 1 2 1 0 2 0 1 1 0 1 2 0 1 1 1 1 1 2 0 0 1
 1 2 0 0 1 0 2 1 1 0 2 1 0 0 1 0 2 1 0 2 0 1 0 1 0 0 0 1 1 2 0 0 1 0 2 1 1
 0 1 1 1 2 1 2 2 0 1 1 0 0 2 1 2 1 0 1 0 2 0 1 1 0 2 2 1 0 1 1 2 0 2 1 1 1
 2 0 1 1 0 2 1 1 0 1 1 2 1 1 0 0 0 0 2 2 0 0 1 0 1 1 1 1 2 0 0 0 1 1 1 2 2
 1 1 1 0 0 0 1 2 0 1 0 1 1 1 2 2 2 0 1 0 0 1 2 0 0 2 2 1 0 0 2 1 0 0 2 0 1
 2 0 0 1 1 2 1 2 1 0 2 1 2 0 2 0 0 1 1 1 2 0 0 0 2 1 1 0 0 0 1 1 0 1 0 0 1
 2 1 1 2 0 0 1 1 0 2 1 1 2 0 2 2 0 0 2 1 1 2 2 1 0 0 0 2 2 0 2 2 2 0 0 0 0
 2 0 2 2 2 2 2 0 1 0 2 2 2 1 2 2 0 0 1 1 2 2 0 2 1 1 1 1 1 1 1 2 1 1 1 1 0
 2 0 0 2 1 0 2 0 2 2 0 2 0 1 1 2 0 1 0 0 1 0 0 1 0 2 1 1 0 0 1 2 2 0 2 1 1
 2 1 2 0 2 1 1 0 0 0 1 1 1 2 0 2 1 0 1 2 0 1 1 0 1 2 0 2 2 0 1 1 0 1 0 0 2
 2 2 1 0 0 1 2 1 0 1 1 0 2 2 2 0 1 2 1 0 0 1 1 0 2 1 2 2 0 2 2 2 1 1 0 2 0
 1 2 0 0 2 0 0 1 1 0 0 2 2 1 0 1 1 0]
clusters: first classes: 307, second classes: 375, thrid classes: 298
```

​		可视化处理后的结果：

![](.\图片\2维数据K=3的K-means聚类.png)

<center>图10：K=3的K-means聚类</center>

##### 3.1.5 二维下K-means结果分析

​		从图9和图10中我们可以分别看到K=2和K=3时的结果，直观上两种K值下的聚类处理的作用都很明显。

​		为了更加精确地分析，我们可以通过**轮廓系数S**来判断聚类的好坏。

![](.\图片\轮廓系数计算公式.png)

​		其计算公式如图所示，其中，a表示样本点与同一簇中所有其他点的平均距离，即样本点与同一簇中其他点的相似度；b表示样本点与下一个最近簇中所有点的平均距离，即样本点与下一个最近簇中其他点的相似度。K-Means追求的是对于每个簇而言，其簇内差异小，而簇外差异大，轮廓系数S正是描述簇内外差异的关键指标。由公式可知，S取值范围为（-1, 1），当S越接近于1，则聚类效果越好，越接近-1，聚类效果越差。

​		当K=2时，通过如下代码计算S值：

```python
from sklearn.datasets import make_blobs
from sklearn.metrics import silhouette_score
cluster = KMeans(n_clusters=2).fit(newdata_2)
pred = cluster.labels_
silhouette_avg = silhouette_score(newdata_2, pred, metric='cosine')
print(silhouette_avg) #结果为0.6596165101353542
```

​		当K=3时，通过如下代码计算S值：

```python
from sklearn.datasets import make_blobs
from sklearn.metrics import silhouette_score
cluster = KMeans(n_clusters=3).fit(newdata_2)
pred = cluster.labels_
silhouette_avg = silhouette_score(newdata_2, pred, metric='cosine')
print(silhouette_avg) #结果为0.5933392248658654
```

​		对比可以发现：当K=2时，轮廓系数S更加接近1，也就意味着K=2时簇内外差异更大，聚类效果更好，这也符合之前SSE值分析的预期结果。

#### 3.2 三维下的聚类处理

##### 3.2.1降维处理

为了控制变量，我们仍然采用主成分分析（PCA）的方法，该算法可以通过正交变换将一组相关变量的观测值转换为一组主成分，从而降低维数。

``` python
pca = PCA(n_components=3)
newdata_3 = pca.fit_transform(data)
```

为了便于观察，将数据延Z轴染不同色，并对处理好的数据进行查看：

```
ax = plt.axes(projection='3d')
ax.scatter3D(x_3, y_3, z_3, c=z_3, cmap='Greens')
ax.set_xlabel('X')
ax.set_ylabel('Y')
ax.set_zlabel('Z')
```

![image-20210519125654612](.\图片\3维处理后的散点图.png)

##### 3.2.2 K=2的K-means聚类

给定K个聚类数，K-均值聚类算法概述如下:

``` 
▪ Select K points as initial centroids
▪ Repeat 
	▪ Form K clusters by assigning each point to its closest centroid 
	▪ Re-compute the centroids (i.e., mean point) of each cluster 
▪ Until convergence criterion is satisfied
```

具体处理过程与二维情况类似，此处仅展示结果如下。可以看出，三维情况下聚类情况尚可，直观上存在一定的离群点，但可能是由于观察角度与聚类切割的平面不同导致的，同时，两个类数量分布比较均匀，与二维情况类似（一个class为565，另一个为415，可参见notebook）。

![image-20210519130455789](.\图片\3维数据K=2的K-means聚类.png)

##### 3.2.3 K=3的K-means聚类

使n_clusters=3，进行相似的处理，结果如下。

旋转一定的视图角度：

```
ax.view_init(60, 35)
```

可以看出效果良好，但是不同簇类之间存在一定交集。

![image-20210519130559395](.\图片\3维数据K=3的K-means聚类.png)

##### 3.2.4 三维下k-means聚类结果分析

采用相同的评估标准轮廓系数：

```
cluster = KMeans(n_clusters=2).fit(newdata_3)
pred = cluster.labels_
silhouette_avg = silhouette_score(newdata_3, pred, metric='cosine')
print(silhouette_avg) 
#K=2：0.5416356745195819
#K=3：0.46850698162585275
```

可以明显地看出，在三维情况下数据的轮廓系数较小，聚类情况不如二维下集中，这在第一反应下是违反直觉的，因为三个维度理应保存更多的信息量，被压缩得更少，也不远远达不到维数爆炸的情况。同时，两种情况的轮廓系数都不高，尤其是在计算S时没有设置计算余弦相似度而是欧式距离的时候。在查阅了一些资料之后，我并没有找到合理的解答，我认为这可能是数据集本身自带类别不明显导致的，在降维之后数据有一部分成中心的团簇所以分类时效果比较差。我们虽然取得了初步的成果，但是仍然有需要改进的地方。

#### 3.3 二维下的dbscan聚类处理

##### 3.3.1降维处理

仍然采用主成分分析（PCA）的方法，该算法可以通过正交变换将一组相关变量的观测值转换为一组主成分，从而降低维数。

``` python
pca = PCA(n_components=2)
newdata4 = pca.fit_transform(data)
```

对处理好的数据进行查看：

``` 
x = []
y = []
for i in range(len(newdata4)):
    x.append(newdata4[i][0])
    y.append(newdata4[i][1])
plt.scatter(x, y)
plt.show()
```

![](D:\STUDY\数据挖掘\Data-Mining-main\聚类\图片\2维处理后的散点图.png)


##### 3.3.2 eps=0.3, min_samples=11的dbscan聚类

构建模型：

```
db = DBSCAN(eps=0.3, min_samples=11)
db.fit(newdata4)
DBSCAN(algorithm='auto', eps=0.3, leaf_size=30, metric='euclidean', metric_params=None, min_samples=10, n_jobs=None,
       p=None)
print('result:', db.labels_)
```

对聚类后的结果进行查看：

```
result: [ 0 -1  0  0  0 -1 -1  0  0  0 -1  0  0  0 -1  0  0 -1  0  0  0  0 -1  0
  0 -1 -1  0 -1 -1  0 -1  0 -1  0  0  0  0  0  0  0 -1  0  0  0 -1 -1  0
  0 -1  0  0 -1  0  0  0  0  0 -1  0 -1 -1  0  0  0  0  0  0  0  0  0 -1
  0  0 -1 -1  0  0  0  0 -1  0  0  0 -1  0  0 -1 -1  0  0  0  0 -1 -1  0
  0  0  0  0  0  0  0  0 -1  0 -1 -1  0  0  0  0  0  0 -1 -1  0  0  0  0
  0  0 -1 -1  0  0  0  0  0  0  0  0  0  0  0 -1  0  0  0 -1  0  0  0  0
 -1  0  0 -1  0  0  0  0  0  0  0  0  0 -1  0 -1  0  1  0  0  0  0  0  0
 -1  0 -1 -1  0  0  0  0  0  0  0  0 -1  0  0  0 -1  0  0 -1 -1 -1 -1  0
 -1  0  0  0  0 -1  0  0 -1  0  0  0  0 -1  0  0 -1  0  0  0  0  0 -1  0
 -1  0 -1  0  0  0 -1  0  0  0 -1  0  0  0 -1 -1 -1 -1  0  0  0  0  0 -1
  0  0  0  0 -1  0 -1 -1  0  0  0  0 -1  0 -1  0 -1  0 -1 -1  0 -1 -1 -1
  0  0 -1  0  0  0  0  0 -1  0  0 -1  0 -1  0  0  0  0 -1  0  0  0  0  0
  0  0 -1  0  0 -1 -1  0  0  0  2  0  0 -1  0  0  0  0  0 -1 -1 -1  0  0
  0  0  0  0  0  0  0 -1 -1 -1  0  0  0  0 -1  0  0  0  0 -1  0  0  0  0
 -1 -1 -1  0  0 -1  0 -1  0  0  2  0  0  0  0  0 -1  0 -1  0  0  0 -1  0
  0  0 -1 -1  0 -1  2  0  0  0  0 -1  0 -1  0 -1 -1  0  0  0  1  1  0  0
 -1 -1  0  0  0  0  0 -1 -1  0 -1  0  0  0  0  0  0  0  0  0  0  0  0  0
  0 -1  0  0 -1  0  2 -1  0 -1  0 -1 -1  0 -1  0  0 -1 -1  0  0  0  0 -1
 -1 -1 -1  0  0  2  0  0  0 -1  0  0  0 -1 -1  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0 -1  0  0  0  0 -1  0  0  0  0 -1  0 -1  0 -1
  0  0  0 -1  0 -1  0 -1  0  0  0 -1  0  0  0  0  0  0  0  0 -1  0  0  0
  0  0  0  0  0 -1 -1 -1  0 -1  0 -1 -1  0  0 -1  0 -1 -1  0 -1  0  0  0
  0  0  0 -1  0  0  0  0 -1  0  0  0  0  0  0  0  0  0  0  0  0  0  0 -1
 -1  0 -1  0  0  0 -1 -1  0 -1  0 -1  0  0 -1  0 -1  0 -1  1  0  0  2  0
  0  0  0  0  0  0  0  0  0 -1  0  0 -1  0  0  0  0  0  0 -1  0  0  0  0
  0  2 -1 -1 -1  0 -1  0  0  0  0  0 -1 -1 -1 -1  0 -1 -1  0  0  0  0  0
  0 -1  0  0  0  0  0 -1  0  0  0  0  0  0  0  0 -1 -1  0  0 -1  0  0 -1
  0  0 -1 -1  0  0 -1 -1  0  0  0  0 -1  0  0  0 -1  0 -1  0 -1  0 -1 -1
  0  0  0  0 -1 -1  0  0  0  0  0  0  0 -1  0  0  0  0  0  0 -1  0  0  0
  0  0  1 -1  0  0  0  0 -1  0  0  0 -1  0 -1  2 -1  0  0 -1  0 -1 -1 -1
  2 -1 -1  0  0  0  2 -1 -1 -1  0  0  0 -1 -1  0  0 -1  0  0  0  0  0  0
  0  0 -1 -1  0  0 -1  0  0  0 -1 -1 -1  0  0  0  0  0  0  0 -1  0  0 -1
 -1  0  0 -1  0  0  0 -1  0  0  0  1  0  0  0 -1  0 -1  0  0  0  0  0 -1
  0 -1  0 -1  0  0  0  0 -1  0  2  0  0 -1  0  0 -1 -1  2 -1  0  0 -1  0
  0  0  0 -1  0  0  0 -1  0  0  0  0 -1  0  0  0 -1  0  0 -1  0  0  0  0
  0  0  0  0 -1  0  0  0  0 -1  0  0  2  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0 -1  0  0  0  0  0  0  0  1  0  0  0 -1  0  0  0  0  0  0
  0  0  0  0  0  0  0  0 -1 -1  0  0  0  0 -1 -1  0 -1  0  0  0 -1  0  0
 -1  0  0 -1 -1  0 -1  0 -1 -1  0  0  0  0 -1  0  0  0  0 -1  0  0  0  0
  0  0  0  0 -1  0 -1  0  0  0  0  0  0 -1 -1  0  0  0  0 -1  0 -1  0 -1
  0  0  0  0 -1 -1 -1  0  0  0  0 -1  0 -1 -1  0  0  0  0  0]
```

绘图：

```
colors = cm.rainbow(np.linspace(0, 1, 5))
x = []
y = []
c = []
for i in range(len(newdata4)):
    x.append(newdata4[i][0])
    y.append(newdata4[i][1])
    c.append(colors[db.labels_[i]+1])
plt.scatter(x, y, color=c)
plt.show()
```

对聚类后的结果图进行查看：
![](D:\STUDY\数据挖掘\Data-Mining-main\聚类\图片\2维数据eps=0.3_min_samples=11dbscan聚类.png)


##### 3.3.3 eps=0.4, min_samples=11的dbscan聚类

代码：

```
db = DBSCAN(eps=0.4, min_samples=11)
db.fit(newdata4)
DBSCAN(algorithm='auto', eps=0.3, leaf_size=30, metric='euclidean', metric_params=None, min_samples=10, n_jobs=None,
       p=None)
colors = cm.rainbow(np.linspace(0, 1, 5))
x = []
y = []
c = []
for i in range(len(newdata4)):
    x.append(newdata4[i][0])
    y.append(newdata4[i][1])
    c.append(colors[db.labels_[i]+1])
plt.scatter(x, y, color=c)
plt.show()
```

对聚类后的结果图进行查看：
![](D:\STUDY\数据挖掘\Data-Mining-main\聚类\图片\2维数据eps=0.4_min_samples=11dbscan聚类.png)


##### 3.3.4 eps=0.3, min_samples=10的dbscan聚类

代码：

```
db = DBSCAN(eps=0.3, min_samples=10)
db.fit(newdata4)
DBSCAN(algorithm='auto', eps=0.3, leaf_size=30, metric='euclidean', metric_params=None, min_samples=10, n_jobs=None,
       p=None)
colors = cm.rainbow(np.linspace(0, 1, 5))
x = []
y = []
c = []
for i in range(len(newdata4)):
    x.append(newdata4[i][0])
    y.append(newdata4[i][1])
    c.append(colors[db.labels_[i]+1])
plt.scatter(x, y, color=c)
plt.show()
```

对聚类后的结果图进行查看：
![](D:\STUDY\数据挖掘\Data-Mining-main\聚类\图片\2维数据eps=0.3_min_samples=10dbscan聚类.png)

### 4 总结

在这次大作业中，我们选取了有关东南亚旅游爱好者的数据并进行了聚类分析。我们首先了解了数据集的概况，并针对其特征进行了预处理，如删除用户id等无用数据，处理离群点等，再对其进行了 `Z-score`规范化和PCA降维处理。

然后我们采用了三种不同的聚类处理方法并比较它们的优劣：二维和三维的k-means聚类以及二维dbscan的聚类处理，并观察了各算法不同参数的结果。k-means算法和dbscan都是成熟的聚类算法，可以被方便地调用并生成结果供我们分析。k-means算法原理简单，收敛快，参数少，但是对噪音比较敏感，只对凸数据集比较有效；dbscan算法可以用于不同数据集，对异常点不敏感，但是参数比较复杂，而且依赖样本的密度分布。

通过这次实践，我们对常见的聚类算法和数据处理方法有了更深入的认识，并且学会了用计算方法分析得到的结果并进行评估和思考改进的方法。

