<center><h1>文档</h1></center>

[toc]



### 1. 数据集概况

​		本小组采用的数据集采集于 `TripAdvisor.com` ，数据为旅行者对东亚地区10个类别的目的地进行的评估。利用这些数据，可以对用户进行聚类，从而得知哪些用户具有共同的旅游爱好。



### 2. 数据预处理

#### 2.1 了解数据的概况

​		使用 `pandas.read_csv()` 函数导入 `csv` 文件并使用 `info()` 函数查看数据的概况，发现共有11个属性，980条数据。其中第一个属性为 `user id ` ，对数据分析并无用处，所以之后的分析将不对这一列进行处理。

<img src=".\图片\数据概况.png" style="zoom: 80%;" />

<center>图1：数据集概况</center>

可以看到并没有数据丢失，再来看看每个数据的值大概是多少，有没有离群点。先画个散点图粗略地看一看。我们使用了`plt.scatter()` 画图，具体代码和图片如下：

```python
#画出10个属性索引与数值的散点图，观察数据大致的分布
plt.rcParams['figure.figsize'] = (20, 8) #设置画布大小为20*8
fig, ax = plt.subplots(2,5) #设置子图，为2*5方哥摆布
plt.subplots_adjust(wspace=0.2, hspace=0.3)
#画出每个属性的散点图
for i in range(10):
    num = i+1
    plt.subplot(2,5,num)
    plt.scatter(df.index, df[df.columns[i+1]], c="b", label=str(df.columns[i+1]), s=3)
    plt.ylabel("数值")
    plt.xlabel("索引")
    plt.title(df.columns[i+1])
plt.show()
```

![](.\图片\散点图.png)

<center>图2：数据值查看</center>

发现确实存在离群点，接下来就对离群点进行处理。



#### 2.2 离群点的处理

​		我们先仔细观察离群点，使用箱体图具体观察。选用箱体图，是因为箱体图可以反映原始数据分布的特征，并方便的确定离群点。具体代码和图片如下：

```python
#由散点图发现存在离群点，用箱体图能更加细致地查看
#第一个属性"user id"在数据分析中并没有什么作用，所以使用去掉"user id"属性的data进行下一步分析
labels = ["art galleries", "dance clubs", "juice bars", "restaurants", "museums", "resorts", "parks/picnic spots", "beaches", "theaters", "religious institutions"]
data = df[labels]
box = []
#绘制箱体图
for i in range(10):
    box.append(data[data.columns[i]])
plt.title('各属性箱体图',fontsize=20) #标题，并设定字号大小
p = plt.boxplot(box, labels = labels, patch_artist = True, boxprops = {'color':'orangered','facecolor':'pink'})
```

![](.\图片\原数据箱体图.png)

<center>图3：各属性箱体图</center>

​		在箱体图中，具体可以看到第1个属性到第9个属性，或多或少都具有离群点，只有最后一个属性不存在离群点。但是第7个属性数据的分布由图中查看显然非常的集中，统计的离群点应该是有点问题的。浏览了具体的数据后发现是因为数值之间差值很小，又因为取值也就几个，比较集中，所以这个属性的数值没必要做修改。
​		接下来我们对离群点做处理，我们的想法是设置大于上限的值为最大值，小于下限的值为最小值，从而修改离群点为正常点。这里我们设计了一个函数 `BoxFeature()`：用于获得箱体图的具体信息：

```python
#BoxFeature函数用于获取箱体图的具体信息
def BoxFeature(input_list):
    # 获取箱体图特征
    percentile = np.percentile(input_list, (25, 50, 75), interpolation='linear')
    #以下为箱线图的五个特征值
    Q1 = percentile[0] #上四分位数
    Q2= percentile[1]
    Q3 = percentile[2] #下四分位数
    IQR = Q3 - Q1 #四分位距
    ulim = Q3 + 1.5 * IQR #上限 非异常范围内的最大值
    llim = Q1 - 1.5 * IQR #下限 非异常范围内的最小值
    right_list = [] #正常数据列表
    Error_Point_num = 0 #异常点个数
    value_total = 0 
    average_num = 0 #平均值
    for item in input_list:
        if item < llim or item > ulim:
            Error_Point_num += 1
        else:
            right_list.append(item)
        value_total += item
        average_num += 1
    average_value = value_total/average_num
    # 特征值保留一位小数
    out_list = [average_value,min(right_list), Q1, Q2, Q3, max(right_list)]
    # print(out_list)
    return out_list,Error_Point_num
```

​		利用这个函数，我们获得箱体图正常的最大最小值，从而对离群点进行修改，具体代码和修改后的箱体图如下：

```python
#设置大于上限的值为最大值，小于下限的值为最小值，从而修改离群点为正常点
for i in range(10):
    if i == 6: #第7个属性因为数据相当集中，差值较小，不必更改
        continue
    name = data.columns[i]
    #调用BoxFeature函数获得箱体图信息
    out_list,Error_Point_num = BoxFeature(data[name])
    maxnum = out_list[5] 
    minnum = out_list[1]
    for j in range(980):
        if data.iat[j,i]> maxnum :
            data.iat[j,i] = maxnum
        elif data.iat[j,i] < minnum:
            data.iat[j,i] = minnum   
labels = ["art galleries", "dance clubs", "juice bars", "restaurants", "museums", "resorts", "parks/picnic spots", "beaches", "theaters", "religious institutions"]
box = []
for i in range(10):
    box.append(data[data.columns[i]])
plt.title('Examples of boxplot',fontsize=20)#标题，并设定字号大小
p = plt.boxplot(box, labels = labels, patch_artist = True, boxprops = {'color':'orangered','facecolor':'pink'})
```

![](.\图片\处理后箱体图.png)

<center>图4：去除离群点后箱体图</center>

可以发现，除了属性7，其余属性中，已经不存在离群点，效果很好。



#### 2.3 数据规范化

​		接下来，我们对数据进行了归一化，方便后续处理，以获得更好的聚类效果。这里我们采用的是 `Z-score` 规范化。因为我们在图1中其实可以看出来，各属性数值的分布基本是中间集中，往两边逐渐减少，类似正态分布。从图4中也可以看出这样的趋势。不过，从前期可以看出，这些属性里，第三个属性 `juice bars` 的大量数据集中在较小值，可能不符合这个规律。
​		我们对数据使用 `Z-score` 规范化后，再绘制直方图可观察数据分布，具体代码和图像如下：

```python
#规范化数据，方便之后的处理
fig, ax = plt.subplots(2,5)
plt.subplots_adjust(wspace=0.2, hspace=0.3)
values = data.astype(float) #dataframe转换为array
g = preprocessing.scale(values, axis=0, with_mean=True, with_std=True, copy=True)
data = pd.DataFrame(g)
data.columns = ["art galleries", "dance clubs", "juice bars", "restaurants", "museums", "resorts", "parks/picnic spots", "beaches", "theaters", "religious institutions"]
for i in range(10):
    plt.subplot(2,5,i+1)
    plt.hist(data[data.columns[i]], bins=100)
    plt.ylabel("数目")
    plt.xlabel("数值")
    plt.title(data.columns[i])
plt.show()
```

![](.\图片\直方图.png)

<center>图5：各属性分布直方图</center>

从图片中可以观察到，确实除了第3个属性 `juice bars` 不太符合之外，其余属性规范化的效果都很好。



#### 2.4 属性相关性判断

​		为了防止属性冗余，我们需要对各个属性间的相关性进行判断。这里我们采用散点图矩阵方便的判断。具体代码和图片如下：

```python
#查看每个属性之间是否存在关系，由相关性散点图可以看出来，10个属性之间的关系很小
pd.plotting.scatter_matrix(data, figsize=(20,20), c = 'k', marker = '0', s = 3, diagonal='hist', alpha = 0.8, range_padding=0.2)
plt.show()
```

![](.\图片\散点矩阵.png)

<center>图6：散点图矩阵</center>

从散点图矩阵中，我们可以发现，各个属性的数值之间并没有呈现出线性的关系，可以得出结论：属性间的相关性小，没有冗余的属性。



### 3.聚类处理

#### 3.1二维下的聚类处理

##### 3.1.1降维处理

​		出于减少计算量，进而提高机器运作效率的目的，我们往往会对数据进行降维处理。所以尽管3维数据不是必须要降维，这里还是将数据降到2维，来实验降维的功能。

```python
#进行降维
pca = PCA(n_components=2)
newdata = pca.fit_transform(data)

x = []
y = []

for i in range(len(newdata)):
    x.append(newdata[i][0])
    y.append(newdata[i][1])

plt.scatter(x, y)
plt.show()
#降到2维的结果
```

​		可视化结果如下：

![](.\图片\2维处理后的散点图.png)

<center>图7：降维处理后的散点图</center>

##### 3.1.2 K=2的K-means聚类

​		使用K=2的sklearn K-means对数据进行聚类处理。

```python
km_cluster = KMeans(n_clusters=2, max_iter=100, init='k-means++')
#K=2，最多迭代100次，按k-means++选初始点
predictions = km_cluster.fit_predict(newdata)
print('result:', predictions)
print('clusters: first classes: %d, second classes: %d' %
      (np.sum(predictions == 0), np.sum(predictions == 1)))
#聚类结果，以及每个类包含的数据个数
```

```
result: [0 0 1 1 0 1 1 1 0 1 0 1 0 0 0 0 1 0 0 0 1 1 1 1 0 0 0 0 1 0 0 0 0 0 1 1 1
 1 0 1 1 1 1 1 1 1 1 1 1 0 1 1 0 1 1 1 0 1 0 0 0 1 1 1 1 1 1 1 1 1 1 0 1 1
 0 0 1 0 1 1 1 1 1 1 1 1 0 1 0 1 1 1 0 0 0 1 0 1 0 1 1 1 0 1 1 0 0 1 1 0 0
 1 0 0 0 0 0 1 1 1 0 0 0 0 0 1 1 0 1 1 1 0 0 0 1 0 0 0 1 1 1 1 1 1 1 1 0 0
 0 0 1 1 1 1 1 1 1 1 0 1 1 1 0 1 1 1 0 1 0 1 0 1 1 1 1 0 1 1 1 1 1 0 0 1 0
 1 1 1 0 1 0 0 1 1 1 1 1 0 1 0 0 1 0 1 1 1 1 1 1 1 1 0 1 1 1 0 1 1 0 0 0 1
 0 1 0 1 0 1 1 0 1 0 1 1 0 1 0 1 0 1 0 0 0 1 1 1 1 0 1 1 0 1 0 0 1 1 1 0 1
 0 0 0 1 0 1 0 0 0 1 0 1 1 0 0 0 1 0 0 1 1 1 1 0 0 1 1 0 1 1 1 0 1 1 1 1 0
 1 1 0 0 1 0 0 1 0 1 1 0 1 0 1 1 1 0 0 1 0 1 0 1 1 1 0 1 1 0 0 0 0 0 1 0 1
 0 1 0 0 1 0 1 0 0 1 0 1 1 0 1 0 0 1 1 0 1 1 1 0 1 0 1 1 0 0 1 0 0 0 1 1 0
 1 0 0 0 1 0 0 1 1 0 1 1 0 1 1 0 0 0 1 1 0 1 0 1 1 0 1 1 1 0 1 0 1 1 0 0 1
 0 0 0 1 0 0 0 0 0 1 1 1 0 0 1 0 1 1 0 0 1 1 1 0 0 0 0 0 1 1 0 1 1 1 1 1 1
 0 1 0 1 0 0 1 1 1 0 1 1 1 1 0 1 1 1 1 1 1 0 1 1 1 1 0 1 1 0 1 0 1 1 0 0 1
 0 1 0 0 1 1 1 1 0 1 1 1 0 1 1 1 0 0 1 0 1 1 1 1 1 1 0 0 1 0 1 1 0 1 0 1 0
 1 1 1 0 0 1 0 0 0 1 1 1 0 1 0 1 1 1 0 0 0 1 1 1 1 1 0 1 1 1 0 1 1 0 1 0 0
 0 1 1 1 1 0 0 0 1 0 1 0 0 0 1 0 1 0 1 0 0 1 1 1 0 1 0 0 1 1 1 1 1 0 1 1 1
 1 0 1 1 1 0 0 1 1 0 0 1 0 0 1 1 0 1 1 0 1 1 1 1 1 1 1 1 1 0 1 1 1 1 0 1 1
 1 1 1 1 0 1 0 0 0 1 1 0 0 0 1 0 1 1 1 1 0 1 1 1 1 0 0 1 0 1 1 0 1 0 1 1 1
 0 0 1 1 1 0 1 1 1 1 1 0 1 1 1 1 0 1 0 0 1 1 1 0 1 1 1 1 0 0 1 1 1 1 1 0 0
 1 1 1 1 0 1 1 0 0 1 1 1 1 1 0 0 0 0 1 0 1 1 0 0 0 0 0 1 1 1 0 1 1 0 0 0 1
 0 0 1 1 1 0 1 0 1 1 0 1 0 1 0 1 1 1 1 1 0 0 1 1 0 1 1 0 0 1 1 1 1 1 1 0 1
 0 1 1 0 0 1 1 1 0 0 1 1 0 1 0 0 1 1 0 1 1 0 0 1 0 0 1 0 0 1 0 0 0 0 0 0 1
 0 1 0 0 0 0 0 1 1 0 0 0 0 1 0 0 1 1 1 1 0 0 0 0 1 1 1 1 1 1 1 0 1 1 1 1 1
 0 0 0 0 1 0 0 1 0 0 1 0 1 1 1 0 0 1 1 0 1 1 1 1 0 0 1 1 0 1 1 0 0 0 0 1 1
 0 1 0 1 0 1 1 1 0 1 1 1 1 0 0 0 1 0 1 0 1 1 1 0 1 0 0 0 0 1 1 1 0 1 1 1 0
 0 0 1 0 1 1 0 1 1 1 1 1 0 0 0 0 1 0 1 1 1 1 1 0 0 1 0 0 0 0 0 0 1 1 1 0 1
 1 0 0 1 0 1 1 1 1 1 1 0 0 1 1 1 1 0]
clusters: first classes: 414, second classes: 566
```

​		可视化处理后的结果：

![](.\图片\2维数据K=2的K-means聚类.png)

<center>图8：K=2的K-means聚类</center>

​		从图像中观察可以看到K=2的K-means聚类结果良好。

##### 3.1.2 K=3的K-means聚类

​		使用K=3的sklearn K-means对数据进行聚类处理。

```python
km_cluster = KMeans(n_clusters=3, max_iter=300, init='k-means++')
#K=3，最多迭代300次，按k-means++选初始点
predictions = km_cluster.fit_predict(newdata)
print('result:', predictions)
print('clusters: first classes: %d, second classes: %d, thrid classes: %d' %
      (np.sum(predictions == 0), np.sum(predictions == 1), np.sum(predictions == 2)))
#聚类结果，以及每个类包含的数据个数
```

```
result: [2 2 0 1 2 1 1 1 2 1 0 1 2 2 2 2 1 2 0 2 1 1 1 1 0 2 2 2 1 2 2 2 2 0 0 0 1
 1 2 0 1 1 1 0 0 1 0 1 1 2 1 1 2 0 1 1 2 1 2 2 2 1 1 1 0 0 1 1 1 1 0 2 1 1
 2 2 1 2 1 1 0 0 1 0 0 1 0 1 2 1 0 1 2 2 2 0 2 0 0 0 0 1 2 1 1 2 0 1 1 2 0
 1 0 0 2 0 2 0 1 1 2 0 2 0 2 1 1 2 1 1 1 2 2 2 1 0 2 2 1 1 1 1 1 0 0 1 0 2
 0 2 0 1 1 1 1 1 0 1 2 1 1 1 0 1 1 1 2 0 2 0 0 0 1 1 0 2 0 0 0 0 0 2 0 1 2
 0 0 0 2 1 2 2 1 1 1 1 1 2 1 2 2 0 2 0 1 1 1 0 1 1 1 2 1 1 1 2 1 1 2 0 0 1
 2 1 2 1 2 1 1 2 0 2 1 1 0 1 0 0 0 1 2 2 0 1 1 1 0 2 1 1 2 1 2 2 1 0 1 2 0
 2 0 0 0 0 1 2 2 2 1 2 1 0 2 2 2 1 2 0 1 0 1 0 2 2 1 1 2 0 1 1 2 1 1 0 1 0
 1 1 0 2 1 2 2 1 0 1 0 2 0 2 1 1 1 2 2 0 2 1 2 1 0 0 2 1 0 2 2 2 2 0 0 2 0
 0 1 2 2 1 0 0 2 2 0 2 1 1 0 0 2 2 0 0 0 1 1 1 0 1 0 1 1 2 2 1 0 2 0 0 0 2
 1 2 0 2 0 2 2 1 0 2 1 1 0 1 1 2 2 0 1 1 2 1 2 1 0 2 1 0 1 0 0 2 0 1 2 2 1
 2 2 2 1 2 2 2 0 2 1 1 1 2 2 1 2 1 1 0 2 1 1 1 0 0 2 2 2 1 1 0 0 0 1 1 1 0
 2 1 2 1 2 0 1 1 0 2 0 1 0 1 2 1 0 1 0 0 0 2 1 0 1 1 2 1 1 0 1 0 1 1 2 2 0
 2 1 2 2 1 0 0 1 0 1 1 1 2 0 1 1 2 0 0 2 0 1 0 0 1 0 2 2 1 2 1 0 2 1 2 0 2
 1 0 1 2 2 1 2 2 2 0 1 0 0 0 2 1 1 1 2 2 2 0 1 1 0 1 2 1 1 0 2 1 0 2 0 0 2
 2 1 0 1 0 2 2 0 1 0 1 2 2 2 1 2 1 2 1 0 2 0 1 1 0 1 2 0 1 1 1 1 1 2 0 0 1
 1 2 0 0 1 0 2 1 1 0 2 1 0 0 1 0 2 1 0 2 0 1 0 1 0 0 0 1 1 2 0 0 1 0 2 1 1
 0 1 1 1 2 1 2 2 0 1 1 0 0 2 1 2 1 0 1 0 2 0 1 1 0 2 2 1 0 1 1 2 0 2 1 1 1
 2 0 1 1 0 2 1 1 0 1 1 2 1 1 0 0 0 0 2 2 0 0 1 0 1 1 1 1 2 0 0 0 1 1 1 2 2
 1 1 1 0 0 0 1 2 0 1 0 1 1 1 2 2 2 0 1 0 0 1 2 0 0 2 2 1 0 0 2 1 0 0 2 0 1
 2 0 0 1 1 2 1 2 1 0 2 1 2 0 2 0 0 1 1 1 2 0 0 0 2 1 1 0 0 0 1 1 0 1 0 0 1
 2 1 1 2 0 0 1 1 0 2 1 1 2 0 2 2 0 0 2 1 1 2 2 1 0 0 0 2 2 0 2 2 2 0 0 0 0
 2 0 2 2 2 2 2 0 1 0 2 2 2 1 2 2 0 0 1 1 2 2 0 2 1 1 1 1 1 1 1 2 1 1 1 1 0
 2 0 0 2 1 0 2 0 2 2 0 2 0 1 1 2 0 1 0 0 1 0 0 1 0 2 1 1 0 0 1 2 2 0 2 1 1
 2 1 2 0 2 1 1 0 0 0 1 1 1 2 0 2 1 0 1 2 0 1 1 0 1 2 0 2 2 0 1 1 0 1 0 0 2
 2 2 1 0 0 1 2 1 0 1 1 0 2 2 2 0 1 2 1 0 0 1 1 0 2 1 2 2 0 2 2 2 1 1 0 2 0
 1 2 0 0 2 0 0 1 1 0 0 2 2 1 0 1 1 0]
clusters: first classes: 307, second classes: 375, thrid classes: 298
```

​		可视化处理后的结果：

![](.\图片\2维数据K=3的K-means聚类.png)

<center>图9：K=3的K-means聚类</center>

​		从图像中观察可以看到K=3的K-means聚类结果良好。

#### 3.2 三维下的聚类处理

##### 3.2.1降维处理

为了控制变量，我们仍然采用主成分分析（PCA）的方法，该算法可以通过正交变换将一组相关变量的观测值转换为一组主成分，从而降低维数。

``` python
pca = PCA(n_components=3)
newdata_3 = pca.fit_transform(data)
```

对处理好的数据进行查看：

```
ax = plt.axes(projection='3d')
ax.scatter3D(x_3, y_3, z_3, c=z_3, cmap='Greens')
ax.set_xlabel('X')
ax.set_ylabel('Y')
ax.set_zlabel('Z')
```

![image-20210519125654612](.\图片\3维处理后的散点图.png)

##### 3.2.2 K=2的K-means聚类

给定K个聚类数，K-均值聚类算法概述如下:

``` 
▪ Select K points as initial centroids
▪ Repeat 
	▪ Form K clusters by assigning each point to its closest centroid 
	▪ Re-compute the centroids (i.e., mean point) of each cluster 
▪ Until convergence criterion is satisfied
```

具体处理过程与二维情况类似，此处仅展示结果如下。可以看出，三维情况下聚类情况尚可，直观上存在一定的离群点，但可能是由于观察角度与聚类切割的平面不同导致的。

![image-20210519130455789](.\图片\3维数据K=2的K-means聚类.png)

##### 3.2.3 K=3的K-means聚类

使n_clusters=3，进行相似的处理，结果如下。

旋转一定的试图角度：

```
ax.view_init(60, 35)
```

可以看出效果良好，分界更加明显。

![image-20210519130559395](.\图片\3维数据K=3的K-means聚类.png)


#### 3.3 二维下的dbscan聚类处理

##### 3.3.1降维处理

仍然采用主成分分析（PCA）的方法，该算法可以通过正交变换将一组相关变量的观测值转换为一组主成分，从而降低维数。

``` python
pca = PCA(n_components=2)
newdata4 = pca.fit_transform(data)
```

对处理好的数据进行查看：
``` 
x = []
y = []
for i in range(len(newdata4)):
    x.append(newdata4[i][0])
    y.append(newdata4[i][1])
plt.scatter(x, y)
plt.show()
```

![](.\图片\2维处理后的散点图.png)


##### 3.3.2 eps=0.3, min_samples=11的dbscan聚类

构建模型：
```
db = DBSCAN(eps=0.3, min_samples=11)
db.fit(newdata4)
DBSCAN(algorithm='auto', eps=0.3, leaf_size=30, metric='euclidean', metric_params=None, min_samples=10, n_jobs=None,
       p=None)
print('result:', db.labels_)
```

对聚类后的结果进行查看：
```
result: [ 0 -1  0  0  0 -1 -1  0  0  0 -1  0  0  0 -1  0  0 -1  0  0  0  0 -1  0
  0 -1 -1  0 -1 -1  0 -1  0 -1  0  0  0  0  0  0  0 -1  0  0  0 -1 -1  0
  0 -1  0  0 -1  0  0  0  0  0 -1  0 -1 -1  0  0  0  0  0  0  0  0  0 -1
  0  0 -1 -1  0  0  0  0 -1  0  0  0 -1  0  0 -1 -1  0  0  0  0 -1 -1  0
  0  0  0  0  0  0  0  0 -1  0 -1 -1  0  0  0  0  0  0 -1 -1  0  0  0  0
  0  0 -1 -1  0  0  0  0  0  0  0  0  0  0  0 -1  0  0  0 -1  0  0  0  0
 -1  0  0 -1  0  0  0  0  0  0  0  0  0 -1  0 -1  0  1  0  0  0  0  0  0
 -1  0 -1 -1  0  0  0  0  0  0  0  0 -1  0  0  0 -1  0  0 -1 -1 -1 -1  0
 -1  0  0  0  0 -1  0  0 -1  0  0  0  0 -1  0  0 -1  0  0  0  0  0 -1  0
 -1  0 -1  0  0  0 -1  0  0  0 -1  0  0  0 -1 -1 -1 -1  0  0  0  0  0 -1
  0  0  0  0 -1  0 -1 -1  0  0  0  0 -1  0 -1  0 -1  0 -1 -1  0 -1 -1 -1
  0  0 -1  0  0  0  0  0 -1  0  0 -1  0 -1  0  0  0  0 -1  0  0  0  0  0
  0  0 -1  0  0 -1 -1  0  0  0  2  0  0 -1  0  0  0  0  0 -1 -1 -1  0  0
  0  0  0  0  0  0  0 -1 -1 -1  0  0  0  0 -1  0  0  0  0 -1  0  0  0  0
 -1 -1 -1  0  0 -1  0 -1  0  0  2  0  0  0  0  0 -1  0 -1  0  0  0 -1  0
  0  0 -1 -1  0 -1  2  0  0  0  0 -1  0 -1  0 -1 -1  0  0  0  1  1  0  0
 -1 -1  0  0  0  0  0 -1 -1  0 -1  0  0  0  0  0  0  0  0  0  0  0  0  0
  0 -1  0  0 -1  0  2 -1  0 -1  0 -1 -1  0 -1  0  0 -1 -1  0  0  0  0 -1
 -1 -1 -1  0  0  2  0  0  0 -1  0  0  0 -1 -1  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0 -1  0  0  0  0 -1  0  0  0  0 -1  0 -1  0 -1
  0  0  0 -1  0 -1  0 -1  0  0  0 -1  0  0  0  0  0  0  0  0 -1  0  0  0
  0  0  0  0  0 -1 -1 -1  0 -1  0 -1 -1  0  0 -1  0 -1 -1  0 -1  0  0  0
  0  0  0 -1  0  0  0  0 -1  0  0  0  0  0  0  0  0  0  0  0  0  0  0 -1
 -1  0 -1  0  0  0 -1 -1  0 -1  0 -1  0  0 -1  0 -1  0 -1  1  0  0  2  0
  0  0  0  0  0  0  0  0  0 -1  0  0 -1  0  0  0  0  0  0 -1  0  0  0  0
  0  2 -1 -1 -1  0 -1  0  0  0  0  0 -1 -1 -1 -1  0 -1 -1  0  0  0  0  0
  0 -1  0  0  0  0  0 -1  0  0  0  0  0  0  0  0 -1 -1  0  0 -1  0  0 -1
  0  0 -1 -1  0  0 -1 -1  0  0  0  0 -1  0  0  0 -1  0 -1  0 -1  0 -1 -1
  0  0  0  0 -1 -1  0  0  0  0  0  0  0 -1  0  0  0  0  0  0 -1  0  0  0
  0  0  1 -1  0  0  0  0 -1  0  0  0 -1  0 -1  2 -1  0  0 -1  0 -1 -1 -1
  2 -1 -1  0  0  0  2 -1 -1 -1  0  0  0 -1 -1  0  0 -1  0  0  0  0  0  0
  0  0 -1 -1  0  0 -1  0  0  0 -1 -1 -1  0  0  0  0  0  0  0 -1  0  0 -1
 -1  0  0 -1  0  0  0 -1  0  0  0  1  0  0  0 -1  0 -1  0  0  0  0  0 -1
  0 -1  0 -1  0  0  0  0 -1  0  2  0  0 -1  0  0 -1 -1  2 -1  0  0 -1  0
  0  0  0 -1  0  0  0 -1  0  0  0  0 -1  0  0  0 -1  0  0 -1  0  0  0  0
  0  0  0  0 -1  0  0  0  0 -1  0  0  2  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0 -1  0  0  0  0  0  0  0  1  0  0  0 -1  0  0  0  0  0  0
  0  0  0  0  0  0  0  0 -1 -1  0  0  0  0 -1 -1  0 -1  0  0  0 -1  0  0
 -1  0  0 -1 -1  0 -1  0 -1 -1  0  0  0  0 -1  0  0  0  0 -1  0  0  0  0
  0  0  0  0 -1  0 -1  0  0  0  0  0  0 -1 -1  0  0  0  0 -1  0 -1  0 -1
  0  0  0  0 -1 -1 -1  0  0  0  0 -1  0 -1 -1  0  0  0  0  0]
```

绘图：
```
colors = cm.rainbow(np.linspace(0, 1, 5))
x = []
y = []
c = []
for i in range(len(newdata4)):
    x.append(newdata4[i][0])
    y.append(newdata4[i][1])
    c.append(colors[db.labels_[i]+1])
plt.scatter(x, y, color=c)
plt.show()
```

对聚类后的结果图进行查看：
![](.\图片\2维数据eps=0.3_min_samples=11dbscan聚类.png)


##### 3.3.3 eps=0.4, min_samples=11的dbscan聚类

代码：
```
db = DBSCAN(eps=0.4, min_samples=11)
db.fit(newdata4)
DBSCAN(algorithm='auto', eps=0.3, leaf_size=30, metric='euclidean', metric_params=None, min_samples=10, n_jobs=None,
       p=None)
colors = cm.rainbow(np.linspace(0, 1, 5))
x = []
y = []
c = []
for i in range(len(newdata4)):
    x.append(newdata4[i][0])
    y.append(newdata4[i][1])
    c.append(colors[db.labels_[i]+1])
plt.scatter(x, y, color=c)
plt.show()
```

对聚类后的结果图进行查看：
![](.\图片\2维数据eps=0.4_min_samples=11dbscan聚类.png)


##### 3.3.4 eps=0.3, min_samples=10的dbscan聚类

代码：
```
db = DBSCAN(eps=0.3, min_samples=10)
db.fit(newdata4)
DBSCAN(algorithm='auto', eps=0.3, leaf_size=30, metric='euclidean', metric_params=None, min_samples=10, n_jobs=None,
       p=None)
colors = cm.rainbow(np.linspace(0, 1, 5))
x = []
y = []
c = []
for i in range(len(newdata4)):
    x.append(newdata4[i][0])
    y.append(newdata4[i][1])
    c.append(colors[db.labels_[i]+1])
plt.scatter(x, y, color=c)
plt.show()
```

对聚类后的结果图进行查看：
![](.\图片\2维数据eps=0.3_min_samples=10dbscan聚类.png)