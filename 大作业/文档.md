## 3. 数据预处理

​		数据预处理的目的是处理我们在前程无忧网上爬取到的原始数据，对缺失值补全，整合文本数据等，从而能让后续数据可视化与数据分析的操作更加方便与准确。我们采用了 python 的 pandas 库提供函数进行处理。

#### 3.1 公司种类预处理

​		在前程无忧网上，公司种类指的是公司的一栏共有”外资（欧美）“、”民营公司“、”上市公司“等 11 类，但同时也有部分数据缺失，这里我们将这些缺失的数据作为垃圾数据剔除，以提高模型的准确率。这些公司种类与薪资并非直接挂钩，因此并不需要进行什么数字化。我们直接用 pandas 提供的 get_dummies 函数进行了独热码化。

![](.\图片\公司类别.png)

#### 3.2 薪资预处理

​		从网站上获得的薪资数据只有一个特征，但是往往包括最大薪水和最小薪水，这里我们将其处理成最大薪水、最小薪水、平均薪水三个特征，显然这三个特征并非独立，这里这样处理只是为了后续处理的机动性和方便性。同样，就算是薪水一样有缺失值，毫无疑问，这类数据我们将直接剔除。薪水处理的复杂在于，这个特征理论上应当是一个整数值，但实际前程无忧网上提供了许多种表示方式，如元/日、千/月、万/年、千/年等，这就需要对不同的格式进行不同的处理，需要考虑的因素非常多。在实际的数字化中，我们将其全部化为年薪。如100元/日转化成 100 * 365 = 36500元/年，1000元/月转化成 1000 * 12 = 12000元/年。

#### 3.3 学历要求预处理

​		对于学历要求，原本爬取到的数据掺杂了一些错误数据，信息中将招几人放到了学历一栏，这些数据作为缺失值处理。

![](.\图片\学历要求1.png)

处理完后的学历要求共有 9 类，分别是“初中及以下”，“中技”，“中专”，“高中”，“大专”，“本科”，“硕士”，“博士”和缺失值。由于学历存在高低之分，显然学历越高，薪资的平均水平也会提高。因此，我们根据学历的高低，将学历进行细化的数值化，越高的学历数字越高。这里我们设定“博士”对应 8，“初中及以下”对应 1，而缺失则取中位数 3。所以最终的取值列表为 [1, 2, 3, 4, 5, 6, 7, 8]。

#### 3.4 工作经验预处理

​		工作经验也同样有一些错误数据，信息中存在部分学历和招聘人数当作工作经验的情况，这里我们将其作为缺失值处理。

![](.\图片\工作经验.png)

处理完后的工作经验共有7类，分别是“无需经验”（包括应届生和缺失值），“1年经验”，“2年经验”，“3-4年经验”，“5-7年经验”，“8-9年经验”，“10年以上经验”。同样的，工作经验往往是与薪资直接挂钩的，我们也根据工作经验的高低，将工作经验进行细化的数值化，越高的工作经验数字越高。这里我们直接取要求的年限均值作为数值。例如“5-7年经验”，对应 （5+7）/ 2 = 6；1年对应1。所以最后的数值取值列表为 [0.0, 1.0, 2.0, 3.5, 6.0,  8.5, 10.0]。

#### 3.5 公司规模预处理

​		公司规模主要包括 '少于50人'，'50-150人'，'150-500人'，'500-1000人',   '1000-5000人'，'5000-10000人'，'10000人以上' 这7类，同样，我们认为一家企业的人数与薪资有着一定的正比关系，所以使用人数范围的均值作为处理后的数值。最后的数值取值列表为 [25, 100, 325, 750, 10000, 3000, 7500]。

#### 3.6 岗位地区预处理

​		因为我们是对网站上上海地区的岗位进行爬取，所以基本所有岗位都在上海。但依旧有一些在上海招聘但是岗位实际在外地的情况，这一类被统称为异地招聘。其余的地区包括 '上海-浦东新区', '上海-徐汇区', '上海-闵行区',  '上海-嘉定区', '上海-松江区', '上海-杨浦区', '上海-宝山区', '上海-静安区', '上海-黄浦区', '上海-青浦区', '上海-长宁区', '上海-奉贤区', '上海-普陀区', '上海-金山区', '上海-虹口区', '上海-崇明区'，包括了上海共16个区。但是还是存在一些岗位只填了上海，并没有写具体的区。剔除这些数据难免可惜，所以这里我们将这类岗位也作为一个类别。所以综上一共有18类。显然，将地区分成市区郊区这样与薪资挂钩是可笑而没有意义的，所以，我们依旧使用了 pandas 提供的 get_dummies 函数对其进行了独热码化。

#### 3.7 岗位行业预处理

​		我们爬取的数据中，岗位行业包括种类有，'半导体/芯片', '财务/会计', '餐饮服务', '电子商务', '后端开发', '建筑设计施工', '教育行业', '律师行业', '汽车设计制作', '前端开发', '生产运营', '市场营销', '销售管理', '游戏开发', '制药医疗器械' 共16类。这里我们也一样对其进行了独热码处理。

#### 3.8 公司福利预处理

​		职员福利是较为难以处理的数据。在原始的数据中，一个职位可能存在任意多的员工福利，具体如'五险一金', '餐饮补贴', '股票期权'等。并且，福利的细分种类相当多，极为难以操作。 因此，我们首要需要做的是找出最主要的福利种类。这里我们搜索了所有的数据，将每个职位的福利用 split 函数切割（每个元福利间都会用空格分开），分成一个个元福利，再进行统计技术， 仅留下其中出现次数超过2000的数据，共 24 类，我们以字典的形式给出具体元福利和出现的次数：

```
{'五险一金': 46437, '餐饮补贴': 19488, '股票期权': 3028, '弹性工作': 11153, '年终奖金': 27246, '补充医疗保险': 7118, '带薪年假': 14525, '绩效奖金': 28726, '专业培训': 22069, '节日福利': 12440, '免费班车': 5185, '定期体检': 20321, '交通补贴': 12529, '通讯补贴': 12129, '周末双休': 7991, '做五休二': 7163, '员工旅游': 18060, '包吃': 2459, '补充公积金': 2604, '出国机会': 3537, '包住宿': 2011, '全勤奖': 3478, '高温补贴': 2784, '加班补贴': 2447}
```

​		之后，我们为每一类福利分配一列 0-1 编码的数值化属性。若某职位的福利经过 re 的正则匹配存在该关键福利词子串，则该项设置为 1。

#### 3.9 职位预处理

​		职位预处理是非常难处理的数据。原始数据中的职位名称五花八门，甚至一样的职位也能是不同的名字。所以对其的特征提取相当困难。我们认为神经网络或者有监督的机器学习是最好的办法，但是因为时间有限，这里我们简单处理为关键词提取。首先我们需要找到所有职位名称中出现次数较多的子字符串。所以我们设计了 get_all_substrings 函数用于获取一个字符串的所有子字符串（具体代码见代码附录）。再进行遍历计数，找出出现次数最多的有意义的16个子字符串作为特征，然后进行独热编码。我们以字典的形式给出具体提取的特征和出现的次数：

```
{'工程师': 16069,'设计': 3511,'经理': 9212, '高级': 2298,'销售': 4771,'开发': 6311, '前端': 2265,'市场': 3546, '总': 2359, '主管': 4018,'助理': 2503, '专员': 5424, '项目': 2023, '运营': 4185,'电商': 2396, '法务': 2182}
```

​		之后，我们为每一类职位名称特征分配一列 0-1 编码的数值化属性。若某职位名称特征经过 re 的正则匹配存在该职位名称子串，则该项设置为 1。

## 4.特征分析

特征分析将对数据进行可视化的处理，使其表达直观，并利用一些不同的相关系数探讨不同变量之间的联系。

#### 4.1 总体情况分布

##### 4.1.1公司类型分布

按照先前的划分查看岗位所属公司的分布。可以看出民营公司占据了一半以上的岗位数额，拥有着大量需求，除此之外，外企以及合资企业也有相当一部分，国企则可能由于发布岗位会通过官方途径，所以在网站上的较少。

![公司类型分布](.\图片\公司类型分布.png)

##

在现有的分类下，岗位显得近乎完美地平均，但是仔细观察后可以发现计算机相关行业包括前后端以及游戏开发等内容，因此总占比会大一些，也体现了目前IT行业的岗位需求大。

![职位类型分布](.\图片\职位类型分布.png)

##### 4.1.3 岗位地址分布

## 5. 模型预测

​		模型预测在数据预处理的基础上，利用处理好的all_final.csv中的数据，主要针对职位酬薪与职位类型两项进行预测。


#### 5.1 职位酬薪预测模型

​		本次项目的主要任务为对进行统一处理后的职位薪酬的预测，根据预处理结果我们选取了九种数据, 包括公司种类、薪资、学历要求、工作经验、公司规模、岗位地区、岗位行业、公司福利、职位等。

​		在该任务中，我们采取了几种回归算法，包括 AdaBoost, Random Forest, Bagging, Gradient Boost, SVR, Bayesian Ridge, Elastic Net 和 MLP回归。通过实现分别测试了它们的回归准确率和运行时间。

​		以下是AdaBoost, Random Forest, Bagging, Gradient Boost, SVR, Bayesian Ridge, Elastic Net 和 MLP回归的处理运行结果。

![](.\图片\AdaBoost.png)
![](.\图片\Random_Forest.png)
![](.\图片\Bagging.png)
![](.\图片\Gradient_Boost.png)
![](.\图片\SVR.png)
![](.\图片\Bayesian_Ridge.png)
![](.\图片\Elastic_Net.png)
![](.\图片\MLPRegressor.png)

​		重要函数代码如下，包括两个预测模型共同调用的载入数据函数provider，薪资预测的载入函数salary_provider和薪资预测训练函数train:
```
def provider(filepath="../data/all_final.csv",
             is_regression=False,
             salary_pred="high",
             all_data=False):
    # read data from the csv file 
    df = pd.read_csv(filepath, header=0, encoding="utf-8")
    # print df.head(5)

    # preprocess the data [city, quality, company, job, welfare]
    df_city = df.apply(lambda s: np.argmax(list(s[18:34])), axis=1)
    df_quality = df.apply(lambda s: np.argmax(list(s[1: 11])), axis=1)
    df_com = df.apply(lambda s: np.argmax(list(s[35: 50])), axis=1)
    df_job = df.apply(lambda s: np.argmax(list(s[75: 90])), axis=1)
    df_welfare = df.apply(lambda s: np.sum(list(s[51: 74])), axis=1)

    # merge the properties
    if is_regression:
        data = df.iloc[:, 15:18]
        if salary_pred == "high":
            labels = df.iloc[:, 12]
        elif salary_pred == "low":
            labels = df.iloc[:, 13]
        else:
            labels = df.iloc[:, 12:14]
    else:
        data = df.iloc[:, 12:18]
    # print data.head(5)
    # print labels.head(5)

    data['city'] = df_city.values
    data['quality'] = df_quality.values
    data['company'] = df_com.values
    if is_regression:
        data['job'] = df_job.values
    else:
        labels = df_job.values
    data['welfare'] = df_welfare.values
    print(data.head(2))

    # split the data
    X_train, X_test, Y_train, Y_test = train_test_split(data.values, labels.values, test_size=0.2, random_state=42)
    print(X_train.shape, X_test.shape)
    print(Y_train.shape, Y_test.shape)
    if all_data:
        return data.values, labels.values
    return X_train, X_test, Y_train, Y_test
```

```
def salary_provider(preprocessing="None"):
    X_train, X_test, Y_train, Y_test = provider(is_regression=True)

    # Normalize the label [No sense!!]
    # salary_max, salary_min = np.max(Y_train), np.min(Y_train)
    # Y_train = (Y_train - salary_min) / float(salary_max - salary_min)
    # Y_test = (Y_test - salary_min) / float(salary_max - salary_min)
    if preprocessing == "normalize":
        normalizer = Normalizer()
        X_train = normalizer.fit_transform(X_train)
        X_test = normalizer.fit_transform(X_test)
    elif preprocessing == "minmax":
        minmaxscaler = MinMaxScaler()
        X_train = minmaxscaler.fit_transform(X_train)
        X_test = minmaxscaler.fit_transform(X_test)
    elif preprocessing == "standard":
        standardscale = StandardScaler()
        X_train = standardscale.fit_transform(X_train)
        X_test = standardscale.fit_transform(X_test)
    else:
        pass

    print(Y_test)
    print(Y_train)
    return X_train, X_test, Y_train, Y_test
```

```
def train():
    X_train, X_test, Y_train, Y_test = salary_provider()
    ada = AdaBoostRegressor()
    rf = RandomForestRegressor()
    bagging = BaggingRegressor()
    grad = GradientBoostingRegressor()
    svr = SVR()
    bayes_ridge = BayesianRidge()
    elastic_net = ElasticNet()
    mlp = MLPRegressor(hidden_layer_sizes=(64, 128, 64), max_iter=1000)

    regressors = [ada, rf, bagging, grad, svr, bayes_ridge, elastic_net, mlp]
    regressor_names = ["AdaBoost", "Random Forest", "Bagging",
                       "Gradient Boost", "SVR", "Bayesian Ridge",
                       "Elastic Net", "MLPRegressor"]

    # regressors = [mlp]
    # regressor_names = ["MLP"]

    for regressor, regressor_name in zip(regressors, regressor_names):
        intime = time.time()
        regressor.fit(X_train, Y_train)
        Y_pred = regressor.predict(X_test)
        print(X_train.shape, Y_train.shape)
        print("-----------------------------------")
        print(time.time() - intime)
        print("For Regressor : ", regressor_name)
        print("Mean Absolute Error : ", metrics.mean_absolute_error(Y_test, Y_pred))
        # print "Median Absolute Error : ",metrics.median_absolute_error(Y_test, Y_pred)
        # print "Mean Squared Error : ",metrics.mean_squared_error(Y_test, Y_pred)
        print("R2 Score : ", metrics.r2_score(Y_test, Y_pred))
        print("---------------------------------\n")

        for i in range(5):
            print(Y_pred[i], Y_test[i], X_test[i])
```

#### 5.2 职位类型预测模型

​		本次项目的主要任务为对进行统一处理后的职位类型的预测，根据预处理结果我们选取了九种数据, 包括公司种类、薪资、学历要求、工作经验、公司规模、岗位地区、岗位行业、公司福利、职位等。

​		在该任务中，同样地，我们使用了几种最具代表性的分类算法，包括支持向量机 SVM，感知器，高斯贝叶斯，K 近邻、随机森林、XGBoost和神级网络多层感知器

​		以下是 SVM、LP、GNB、KNN的处理运行结果。

![](.\图片\SVM.png) 
![](.\图片\LP.png)
![](.\图片\GNB.png)
![](.\图片\KNN.png)


​		重要函数代码如下，包括两个预测模型共同调用的载入数据函数provider，类型预测的载入函数job_provider和类型预测训练函数train:
```
def provider(filepath="../data/all_final.csv",
             is_regression=False,
             salary_pred="high",
             all_data=False):
    # read data from the csv file 
    df = pd.read_csv(filepath, header=0, encoding="utf-8")
    # print df.head(5)

    # preprocess the data [city, quality, company, job, welfare]
    df_city = df.apply(lambda s: np.argmax(list(s[18:34])), axis=1)
    df_quality = df.apply(lambda s: np.argmax(list(s[1: 11])), axis=1)
    df_com = df.apply(lambda s: np.argmax(list(s[35: 50])), axis=1)
    df_job = df.apply(lambda s: np.argmax(list(s[75: 90])), axis=1)
    df_welfare = df.apply(lambda s: np.sum(list(s[51: 74])), axis=1)

    # merge the properties
    if is_regression:
        data = df.iloc[:, 15:18]
        if salary_pred == "high":
            labels = df.iloc[:, 12]
        elif salary_pred == "low":
            labels = df.iloc[:, 13]
        else:
            labels = df.iloc[:, 12:14]
    else:
        data = df.iloc[:, 12:18]
    # print data.head(5)
    # print labels.head(5)

    data['city'] = df_city.values
    data['quality'] = df_quality.values
    data['company'] = df_com.values
    if is_regression:
        data['job'] = df_job.values
    else:
        labels = df_job.values
    data['welfare'] = df_welfare.values
    print(data.head(2))

    # split the data
    X_train, X_test, Y_train, Y_test = train_test_split(data.values, labels.values, test_size=0.2, random_state=42)
    print(X_train.shape, X_test.shape)
    print(Y_train.shape, Y_test.shape)
    if all_data:
        return data.values, labels.values
    return X_train, X_test, Y_train, Y_test
```

```
def job_provider(preprocessing="None"):
    X_train, X_test, Y_train, Y_test = provider(is_regression=True)
    print(X_train.shape, Y_train.shape)

    if preprocessing == "normalize":
        normalizer = Normalizer()
        X_train = normalizer.fit_transform(X_train)
        X_test = normalizer.fit_transform(X_test)
    elif preprocessing == "minmax":
        minmaxscaler = MinMaxScaler()
        X_train = minmaxscaler.fit_transform(X_train)
        X_test = minmaxscaler.fit_transform(X_test)
    elif preprocessing == "standard":
        standardscale = StandardScaler()
        X_train = standardscale.fit_transform(X_train)
        X_test = standardscale.fit_transform(X_test)
    else:
        pass

    return X_train, X_test, Y_train, Y_test
```

```
def train():
    X_train, X_test, Y_train, Y_test = job_provider()
    svm = SVC()
    perceptron = Perceptron()
    gnb = GaussianNB()
    knn = KNeighborsClassifier()
    rf = RandomForestClassifier()
    xg = XGBClassifier()
    mlp = MLPClassifier()

    classifiers = [svm, perceptron, gnb, knn, rf, xg, mlp]
    classifier_names = ["SVM", "LP", "GNB", "KNN", "RF", "XGB", "MLP"]
    for classifier, classifier_name in zip(classifiers, classifier_names):
        print("-----------------------------------")
        print(classifier)
        intime = time.time()
        classifier.fit(X_train, Y_train)
        Y_pred = classifier.predict(X_test)
        print(time.time() - intime)
        print("Accuracy for ", classifier_name, " : ", metrics.accuracy_score(Y_test, Y_pred))
        print("-----------------------------------\n")
```

## 6. 个人总结

- **沈琪**

  ​		本项目中我负责数据的预处理。经过上一次聚类作业，我已经对数据的预处理有了一些了解。本次大作业的数据预处理与上次作业又有很大的不同。前一次主要是离群点判断剔除、相关性分析等，这次主要是对于一些中文字符串的复杂处理，包括数值转化、关键词提取等。使用的主要工具是 python 的 pandas、re 库，用于对 csv 文件进行操作。

  ​		在整个数据处理过程中，数据的不规整性异常严重，预处理工作量非常之大。譬如年薪这个数据，理论上应当是一个 int 值，但实际前程无忧网上提供了许多种表示方式，如元/日、千/月、万/年等，这就需要对不同的格式进行不同的处理，需要考虑的因素非常多。而且，还会有很多 nan 等异常数据需要处理，要考虑的地方相当多。此外，还有如职位描述与职位种类等数据，很多都是前程无忧网的用户自定义的字符串，完全无法通过直接的分类或独热等方法进行预处理，因此只能用 split, re 等字符串处理手段提取特征后再进行预处理。

  ​		虽然本项目中遇到了很多苦难，但好在一一解决，收获颇丰！

  ​		最后，感谢范磊老师的指导！

- **胡定伟**

  ​		本项目中我负责数据的模型预测。大作业期间研究了很多开源代码的预测模型方法，针对csv数据处理后的或独热或分类的数据，进行预测模型搭建，调用python所带的预测模型库直接进行调用与使用，使用的主要工具是 python 的 sklearn、xgboost 库中所带的模型分析库，用于构建预测模型对相应数据属性进行预测分析。

  ​		选取分类算法时查阅了很多相关资料，主要是了解如何调用python库的现有分类算法以及各个算法之间的差异、优势，实际调用不需要自己搭建。

  ​		遇到了一些困难，但好在有同组同学的帮助和解惑，最后完成了该部分的任务，学到了很多，对数据挖掘尤其是对模型预测有了更深的了解和实践基础。

  ​		同样，十分感谢范磊老师的指导！








## 7. 代码附录

##### 公司种类预处理

```python
df = pd.read_csv("./all_salary.csv")
company_xinzhi = list(df["公司类别"].unique())
print(company_xinzhi)
onehot_leibie = pd.get_dummies(df["公司类别"], prefix ='公司类别')
print(onehot_leibie.head(5))
df = pd.concat([df,onehot_leibie],axis=1)
df = df.drop("公司类别", axis=1)
df.to_csv('./all_leibie.csv', encoding='utf_8_sig')
print(df.head(5))
```



##### 薪资预处理

```python
df = pd.read_csv("./all_leibie.csv")
minimum = []
maximum = []
average = []
for i in range(len(df['薪资'])):
    tmp = df['薪资'][i]
    if re.compile('年').search(tmp):
        yearTag = 1 # year salary
    elif re.compile('天').search(tmp):
        yearTag = 300 # day salary
    else:
        yearTag = 12 # month salary

    if re.compile('千').search(tmp):
        multiTag = 1000 # count as 1000
    elif re.compile('元').search(tmp):
        multiTag = 1 # count as 1
    else:
        multiTag = 10000 # count as 10000

    numberIndex = max(tmp.find("千"), tmp.find("元"), tmp.find("万"))
    numberString = tmp[0:numberIndex]
    numberList = numberString.split("-")
    if len(numberList) == 1:
        minimum.append(int(float(numberList[0]) * multiTag * yearTag))
        maximum.append(int(float(numberList[0]) * multiTag * yearTag))
        average.append(int(float(numberList[0]) * multiTag * yearTag))
    else:
        minimum.append(int(float(numberList[0]) * multiTag * yearTag))
        maximum.append(int(float(numberList[1]) * multiTag * yearTag))
        average.append(int((float(numberList[0])+float(numberList[1]))/2 * multiTag * yearTag))
# append the maxinum and minimum salary in newDF
df.insert(loc=len(df.columns), column="薪资_最大值", value=maximum)
df.insert(loc=len(df.columns), column="薪资_最小值", value=minimum)
df.insert(loc=len(df.columns), column="薪资_平均值", value=average)
df = df.drop("薪资", axis=1)
print(df.head(5))
df.to_csv('./all_salary.csv', encoding='utf_8_sig')
```



##### 学历要求预处理

```python
df = pd.read_csv("./all_salary.csv")
print(list(df["学历要求"].unique()))
newframe = []
for i in range(len(df['学历要求'])):
    tmp = df['学历要求'][i]
    if tmp == '初中及以下':
        newframe.append(1)
    elif tmp == '中技':
        newframe.append(2)
    elif tmp == '中专':
        newframe.append(3)
    elif tmp == '高中':
        newframe.append(4)
    elif tmp == '大专':
        newframe.append(5)
    elif tmp == '本科':
        newframe.append(6)
    elif tmp == '硕士':
        newframe.append(7)
    elif tmp == '博士':
        newframe.append(8)
    else:
        newframe.append(3)
df.drop("学历要求",axis=1)
df.insert(loc=len(df.columns), column="学历", value=newframe)
print(list(df["学历"].unique()))
df.to_csv('./all_edu.csv', encoding='utf_8_sig')
```



##### 工作经验预处理

```python
df = pd.read_csv("./all_edu.csv")
newframe = []
print(list(df["工作经验"].unique()))
for i in range(len(df['工作经验'])):
    tmp = df['工作经验'][i]
    if re.compile("3-4年经验").search(tmp):
        newframe.append(3.5)
    elif re.compile("5-7年经验").search(tmp):
        newframe.append(6)
    elif re.compile("8-9年经验").search(tmp):
        newframe.append(8.5)
    elif re.compile("10年以上经验").search(tmp):
        newframe.append(10)
    elif re.compile("1年经验").search(tmp):
        newframe.append(1)
    elif re.compile("2年经验").search(tmp):
        newframe.append(2)
    elif re.compile('在校生/应届生').search(tmp):
        newframe.append(0)
    elif re.compile('无需经验').search(tmp):
        newframe.append(0)
    else:
        newframe.append(0)
df = df.drop("工作经验",axis=1)
df.insert(loc=len(df.columns), column="工作经验", value=newframe)
print(list(df["工作经验"].unique()))
df.to_csv('./all_experience.csv', encoding='utf_8_sig')
```



##### 公司规模预处理

```python
df = pd.read_csv("./all_experience.csv")
newframe = []
print(list(df["公司规模"].unique()))
for i in range(len(df['公司规模'])):
    tmp = df['公司规模'][i]
    if re.compile("少于50").search(tmp):
        newframe.append(25)
    elif re.compile("50-150").search(tmp):
        newframe.append(100)
    elif re.compile("150-500").search(tmp):
        newframe.append(325)
    elif re.compile("500-1000").search(tmp):
        newframe.append(750)
    elif re.compile("1000-5000").search(tmp):
        newframe.append(3000)
    elif re.compile("5000-10000").search(tmp):
        newframe.append(7500)
    elif re.compile('10000人以上').search(tmp):
        newframe.append(10000)
    else:
        newframe.append(25)
df = df.drop("公司规模",axis=1)
df.insert(loc=len(df.columns), column="公司规模", value=newframe)
print(list(df["公司规模"].unique()))
df.to_csv('./all_guimo.csv', encoding='utf_8_sig')
```



##### 岗位地区预处理

```python
df = pd.read_csv("./all_guimo.csv")
newframe = []
print(list(df["岗位地区"].unique()))

onehot_leibie = pd.get_dummies(df["岗位地区"])
print(onehot_leibie.head(5))
df = pd.concat([df,onehot_leibie],axis=1)
df = df.drop("岗位地区", axis=1)

df.to_csv('./all_diqu.csv', encoding='utf_8_sig')
```



##### 岗位行业预处理

```python
df = pd.read_csv("./all_diqu.csv")
print(list(df["分类"].unique()))


onehot_leibie = pd.get_dummies(df["分类"])
print(onehot_leibie.head(5))
df = pd.concat([df,onehot_leibie],axis=1)
df = df.drop("分类", axis=1)

df.to_csv('./all_fenlei.csv', encoding='utf_8_sig')
```



##### 公司福利预处理

```python
df = pd.read_csv("./all_fenlei.csv")
s = {}
t = {}
for i in range(len(df['公司福利'])):
    tmp = df["公司福利"][i]
    words = str(tmp).split(' ')
    for j in words:
        if j in s.keys():
            s[j]=s[j]+1
        else:
            s[j]=1
for i in s.keys():
    if s[i]>2000:
        t[i] = s[i]
print(t)
del t["nan"]
print(t)
df["公司福利"] = df["公司福利"].fillna(value=0)
for i in t.keys():
    newframe = []
    for j in range(len(df['公司福利'])):
        tmp = df["公司福利"][j]
        if re.compile(i).search(str(tmp)):
            newframe.append(1)
        else:
            newframe.append(0)
    df.insert(loc=len(df.columns), column=i, value=newframe)
print(df.info())
df.to_csv('./all_fuli.csv', encoding='utf_8_sig')
```



##### 职位预处理

```python
def get_all_substrings(input_string):
  length = len(input_string)
  return [input_string[i:j + 1] for i in range(length) for j in range(i,length)]


df = pd.read_csv("./all_fuli.csv")
s = {}
t = {}
for i in range(len(df['职位'])):
    tmp = df["职位"][i]
    words = get_all_substrings(str(tmp))
    for j in words:
        if j in s.keys():
            s[j]=s[j]+1
        else:
            s[j]=1
for i in s.keys():
    if s[i]>2000:
        t[i] = s[i]
print(t)
for i in t.keys():
    newframe = []
    for j in range(len(df['职位'])):
        tmp = df["职位"][j]
        if re.compile(i).search(str(tmp)):
            newframe.append(1)
        else:
            newframe.append(0)
    df.insert(loc=len(df.columns), column=i, value=newframe)
print(df.info())
df.to_csv('./all_final.csv', encoding='utf_8_sig')
```

